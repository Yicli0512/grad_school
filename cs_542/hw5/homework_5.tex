%% to produce a PDF copy, issue the following command:
%%
%%     pdflatex propositional-logic-examples.tex
%%
%% in the same directory containing the LaTeX style files:
%%


\documentclass[11pt,leqno,fleqn]{article}

\usepackage{graphicx} 
\usepackage{times}              % better fonts for mathematical symbols
\usepackage{bm}                 % unlike \boldmath,

                                % \bm can be used anywhere within math mode
                                
\usepackage{amsfonts}
\usepackage{amsmath}                               
\usepackage[scaled=0.9]{helvet} % makes text a little smaller throughout,
                                % but not the text in math mode.
\usepackage{tikz}
\usetikzlibrary{arrows}

\setlength\hoffset{-5pt}      % horizontal offset, to move text horizontally
\setlength{\textwidth}{4.5in} % try different widths
\setlength\voffset{-5pt}      % vertical offset, to move text vertically
\setlength{\textheight}{7in}  % try different heights

\newcommand{\Hide}[1]{}             % use \Hide{bla bla} to hide ``bla bla''
\newcommand{\code}[1]{\texttt{#1}}  % use \code{...} to produce ASCII chars

\newcommand{\Cov}{\mathrm{Cov}}



\title{CS 542, Spring 2014
       \\[1ex]
       \textbf{Assignment 5}}
\author{Shan Sikdar} 
\date{} % omit date

\begin{document}


\maketitle



\section{ 8.3 }
Show by direct evaluation that this ditribution has the property that  $p(a,b) \not = p(a)p(b)$ but that $p(a,b|c) = p(a|c)p(b|c)$ for both c = 0 and c = 1.\\
By the summation rule from chapter 1: $p(a,b) = \sum_{c \in 0,1} p(a,b,c)$\\
So for example $p(0,0) = \sum_{c \in 0,1} p(0,0,c) = .192 + .144 = .136$. \\
Doing similar calculations gives the results: 
\[
\begin{array}{ c | c | c  }
a & b &p(a,b)
   
\\ \hline 
 0 & 0 &  .336
\\ \hline  
 0 & 1 &  .264
\\ \hline  
 1 & 0 & .256
\\ \hline  
 1 & 1 & .144
\end{array}
\]

Using the summation rule again we can obtain $p(a)  =  \sum_{b \in 0,1} \sum_{c \in 0,1} p(a,b,c)$ and  $p(b)  =  \sum_{a \in 0,1} \sum_{c \in 0,1} p(a,b,c)$\\
So for example: \\
 $p(a = 0) * p(b = 0) = ( \sum_{b \in 0,1} \sum_{c \in 0,1} p(a,b,c) \sum_{a \in 0,1} \sum_{c \in 0,1} p(a,b,c))  = (.192 +.144 + .048 +.216)*(.192 +.144 + .192 + .064) = (.6)* (.592) = .3352 $\\
 Doing similar calculations gives the results: 
\[
\begin{array}{ c | c | c  }
a & b &p(a)p(b)
   
\\ \hline 
 0 & 0 &  .3552
\\ \hline  
 0 & 1 &  .2448
\\ \hline  
 1 & 0 & .2368
\\ \hline  
 1 & 1 & .1632
\end{array}
\]
So we can see that $p(a,b) \not = p(a)p(b)$\\
For conditional probability applying the product rule from chapter 1, we have\\
 $p(c,a,b) = p(a,b|c) p(c) $ which leads to $p(a,b|c) =  \frac{p(a,b,c)}{\sum_a \sum_b p(a,b,c)}$\\
 Using a similar technique you can derive:\\
 $p(a|c) = \frac{\sum_b p(a,b,c)}{\sum_a \sum_b p(a,b,c)}$ and \\
  $p(b|c) = \frac{\sum_a p(a,b,c)}{\sum_a \sum_b p(a,b,c)}$
 
 Solving and plugging in the values gives the following results:\\
  \[
\begin{array}{ c | c | c | c | c  }
a & b & c &p(a,b|c) &p(a|c)*p(b|c)
   
\\ \hline 
 0 & 0 & 0  & .400 & .400
\\ \hline  
 0 & 1 & 0  & .100 & .100
\\ \hline  
 1 & 0 & 0  & .400 & .400
\\ \hline  
 1 & 1 & 0  & .100 & .100
\\ \hline 
 0 & 0 & 1  & .277 & .277
\\ \hline  
 0 & 1 & 1  & .415 & .415
\\ \hline  
 1 & 0 & 1  & .123 & .123
\\ \hline  
 1 & 1 & 1  & .185  & .185
 
\end{array}
\]

Since the last two columns are the same $p(a,b|c) = p(a|c)p(b|c)$ and so theres is conditional independence.

\newpage
\section{8.4}
Evaluate the distributions $p(a)$ $p(b|c)$ and  $p(c|a)$ correspnding to the joint distribution given in table 8.2 Hence show by direct evaluation that $p(a,b,c) = p(a) p(c|a) p(b|c)$. \\
$p(a)$ and $p(b|c)$ were computed in the last problem with the following values:
 \[
\begin{array}{ c | c    }
a & p(a) 
   
\\ \hline 
 0 & .6 
\\ \hline  
 0 & .4

\end{array}
\]

\[
\begin{array}{ c | c | c  }
b & c &p(b|c)
   
\\ \hline 
 0 & 0 &  .8
\\ \hline  
 0 & 1 &  .2
\\ \hline  
 1 & 0 & .4
\\ \hline  
 1 & 1 & .6
\end{array}
\]

Similar to $p(b|c)$ and $p(a|c)$, we can use product rule so \\
$p(a|c) = \frac{\sum_b p(a,b,c)}{\sum_b \sum_c p(a,b,c)}$\\
Calculating the values results in :
\[
\begin{array}{ c | c | c  }
b & c &p(c|a)
   
\\ \hline 
 0 & 0 &  .4
\\ \hline  
 0 & 1 &  .6
\\ \hline  
 1 & 0 & .6
\\ \hline  
 1 & 1 & .4
\end{array}
\]
(sanity check .8 *.6 *.4 = .192) So using all these obtained values and multiplying them together, we can get the values of p(a,b,c) found in table 2 on page 419. Therefore  $p(a,b,c) = p(a) p(c|a) p(b|c)$\\
Thus the corresponding directed graph will be:\\
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
  thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]

  \node[main node] (1) {b};
  \node[main node] (2) [ left of=1] {c};
  \node[main node] (3) [ left of=2] {a};

  \path[every node/.style={font=\sffamily\small}]
    (2) edge node [right] {} (1)

    (3) edge node [right] {} (2);

\end{tikzpicture}

\newpage
\section{8.11}
Given $P(D= 1 | G = 1) = .9$ and $P(D = 0|G = 0)$\\
Show (1) $P(F= 0|D = 0)$ and (2) $P(F=0|D=0,B=0)$\\
Note: Not actually putting the calculations into the writeup or it would take forever to type.\\
Using Bayes Theorem, and marginalizing, and then evaluating  \\
(1) $P(F = 0 | D = 0)$ \\
$= \frac{P(D = 0| F = 0) P(F = 0)}{P(D = 0)} $\\
$= \frac{(\sum_{B,G \in \{ 0,1\}} P(D = 0|G) P(G|B,F)P(B))P(F = 0)}{\sum_{B,G,F \in \{ 0,1\}} P(D=0|G)P(G|B,F)P(B)P(F))} = .213$\\
(2)For the second  probability do not marginalize over B and keep it fixed at its observed value:
(1) $P(F=0|D=0,B=0)$\\
$ = \frac{P(D = 0| F = 0) P(F = 0)}{P(D = 0)}$\\
 $= \frac{(\sum_{G \in \{ 0,1\}} P(D = 0|G) P(G|B,F)P(B))P(F = 0)}{\sum_{G,F \in \{ 0,1\}} P(D=0|G)P(G|B,F)P(B)P(F))} = .110$ \\
Since thr probablilites are lower than the values calcualted in the example it shows that the driver is not very reliable. Also the probabilites are lower because these observations provide alternate explanations as to why the guage should read zero.

\section{8.14}
The energy function (8.42) is $E(x,y) = h \sum_i x_i - \beta \sum_{ij}x_i x_j - \nu \sum x_i y_i$. ($n,h,\beta \geq 0$ and $x_i,y_i \in \{-1,1\}$\\
Setting $h = \beta = 0$ gives $E(x,y) = - N \sum x_i y_i $.  The most probable configuration is when the energy is lowest.\\
This happens when the negative sign in from of $N$ stays, which can only happen if the values of $x_i, y_i$ are either both 1 or both negative 1. Therefore $x_i = y_i$ for all i.

\end{document}

